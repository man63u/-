{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXW+RlrYo/9KuazbapWiuX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/man63u/-/blob/main/Vesper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt1taVqW3pb0",
        "outputId": "6b371817-57c2-4f16-e181-4489cbc50a98"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.2.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.5.1%2Bcu118-cp311-cp311-linux_x86_64.whl (838.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m838.4/838.4 MB\u001b[0m \u001b[31m657.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.5.1+cu118\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.26.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n"
          ]
        }
      ],
      "source": [
        "# 安装 PyTorch 和 Hugging Face Transformers\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install transformers\n",
        "!pip install soundfile  # 用于加载音频文件"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 克隆 Vesper 仓库\n",
        "!git clone https://github.com/HappyColor/Vesper.git\n",
        "!cd Vesper\n",
        "\n",
        "# 下载预训练模型（假设模型权重文件为 vesper-4.pth 或 vesper-12.pth）\n",
        "# 如果没有提供模型权重，可以使用 Hugging Face 的 WavLM 作为替代\n",
        "!wget https://raw.githubusercontent.com/HappyColor/Vesper/refs/heads/master/configs/model_config.py  # 替换为实际下载链接"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aew2Pm5X382V",
        "outputId": "c7803b6d-6f02-401d-a621-a8cfd416ea1c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Vesper'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/50)\u001b[K\rremote: Counting objects:   4% (2/50)\u001b[K\rremote: Counting objects:   6% (3/50)\u001b[K\rremote: Counting objects:   8% (4/50)\u001b[K\rremote: Counting objects:  10% (5/50)\u001b[K\rremote: Counting objects:  12% (6/50)\u001b[K\rremote: Counting objects:  14% (7/50)\u001b[K\rremote: Counting objects:  16% (8/50)\u001b[K\rremote: Counting objects:  18% (9/50)\u001b[K\rremote: Counting objects:  20% (10/50)\u001b[K\rremote: Counting objects:  22% (11/50)\u001b[K\rremote: Counting objects:  24% (12/50)\u001b[K\rremote: Counting objects:  26% (13/50)\u001b[K\rremote: Counting objects:  28% (14/50)\u001b[K\rremote: Counting objects:  30% (15/50)\u001b[K\rremote: Counting objects:  32% (16/50)\u001b[K\rremote: Counting objects:  34% (17/50)\u001b[K\rremote: Counting objects:  36% (18/50)\u001b[K\rremote: Counting objects:  38% (19/50)\u001b[K\rremote: Counting objects:  40% (20/50)\u001b[K\rremote: Counting objects:  42% (21/50)\u001b[K\rremote: Counting objects:  44% (22/50)\u001b[K\rremote: Counting objects:  46% (23/50)\u001b[K\rremote: Counting objects:  48% (24/50)\u001b[K\rremote: Counting objects:  50% (25/50)\u001b[K\rremote: Counting objects:  52% (26/50)\u001b[K\rremote: Counting objects:  54% (27/50)\u001b[K\rremote: Counting objects:  56% (28/50)\u001b[K\rremote: Counting objects:  58% (29/50)\u001b[K\rremote: Counting objects:  60% (30/50)\u001b[K\rremote: Counting objects:  62% (31/50)\u001b[K\rremote: Counting objects:  64% (32/50)\u001b[K\rremote: Counting objects:  66% (33/50)\u001b[K\rremote: Counting objects:  68% (34/50)\u001b[K\rremote: Counting objects:  70% (35/50)\u001b[K\rremote: Counting objects:  72% (36/50)\u001b[K\rremote: Counting objects:  74% (37/50)\u001b[K\rremote: Counting objects:  76% (38/50)\u001b[K\rremote: Counting objects:  78% (39/50)\u001b[K\rremote: Counting objects:  80% (40/50)\u001b[K\rremote: Counting objects:  82% (41/50)\u001b[K\rremote: Counting objects:  84% (42/50)\u001b[K\rremote: Counting objects:  86% (43/50)\u001b[K\rremote: Counting objects:  88% (44/50)\u001b[K\rremote: Counting objects:  90% (45/50)\u001b[K\rremote: Counting objects:  92% (46/50)\u001b[K\rremote: Counting objects:  94% (47/50)\u001b[K\rremote: Counting objects:  96% (48/50)\u001b[K\rremote: Counting objects:  98% (49/50)\u001b[K\rremote: Counting objects: 100% (50/50)\u001b[K\rremote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/42)\u001b[K\rremote: Compressing objects:   4% (2/42)\u001b[K\rremote: Compressing objects:   7% (3/42)\u001b[K\rremote: Compressing objects:   9% (4/42)\u001b[K\rremote: Compressing objects:  11% (5/42)\u001b[K\rremote: Compressing objects:  14% (6/42)\u001b[K\rremote: Compressing objects:  16% (7/42)\u001b[K\rremote: Compressing objects:  19% (8/42)\u001b[K\rremote: Compressing objects:  21% (9/42)\u001b[K\rremote: Compressing objects:  23% (10/42)\u001b[K\rremote: Compressing objects:  26% (11/42)\u001b[K\rremote: Compressing objects:  28% (12/42)\u001b[K\rremote: Compressing objects:  30% (13/42)\u001b[K\rremote: Compressing objects:  33% (14/42)\u001b[K\rremote: Compressing objects:  35% (15/42)\u001b[K\rremote: Compressing objects:  38% (16/42)\u001b[K\rremote: Compressing objects:  40% (17/42)\u001b[K\rremote: Compressing objects:  42% (18/42)\u001b[K\rremote: Compressing objects:  45% (19/42)\u001b[K\rremote: Compressing objects:  47% (20/42)\u001b[K\rremote: Compressing objects:  50% (21/42)\u001b[K\rremote: Compressing objects:  52% (22/42)\u001b[K\rremote: Compressing objects:  54% (23/42)\u001b[K\rremote: Compressing objects:  57% (24/42)\u001b[K\rremote: Compressing objects:  59% (25/42)\u001b[K\rremote: Compressing objects:  61% (26/42)\u001b[K\rremote: Compressing objects:  64% (27/42)\u001b[K\rremote: Compressing objects:  66% (28/42)\u001b[K\rremote: Compressing objects:  69% (29/42)\u001b[K\rremote: Compressing objects:  71% (30/42)\u001b[K\rremote: Compressing objects:  73% (31/42)\u001b[K\rremote: Compressing objects:  76% (32/42)\u001b[K\rremote: Compressing objects:  78% (33/42)\u001b[K\rremote: Compressing objects:  80% (34/42)\u001b[K\rremote: Compressing objects:  83% (35/42)\u001b[K\rremote: Compressing objects:  85% (36/42)\u001b[K\rremote: Compressing objects:  88% (37/42)\u001b[K\rremote: Compressing objects:  90% (38/42)\u001b[K\rremote: Compressing objects:  92% (39/42)\u001b[K\rremote: Compressing objects:  95% (40/42)\u001b[K\rremote: Compressing objects:  97% (41/42)\u001b[K\rremote: Compressing objects: 100% (42/42)\u001b[K\rremote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "Receiving objects:   2% (1/50)\rReceiving objects:   4% (2/50)\rReceiving objects:   6% (3/50)\rReceiving objects:   8% (4/50)\rReceiving objects:  10% (5/50)\rReceiving objects:  12% (6/50)\rReceiving objects:  14% (7/50)\rReceiving objects:  16% (8/50)\rReceiving objects:  18% (9/50)\rReceiving objects:  20% (10/50)\rReceiving objects:  22% (11/50)\rReceiving objects:  24% (12/50)\rReceiving objects:  26% (13/50)\rReceiving objects:  28% (14/50)\rReceiving objects:  30% (15/50)\rReceiving objects:  32% (16/50)\rReceiving objects:  34% (17/50)\rReceiving objects:  36% (18/50)\rReceiving objects:  38% (19/50)\rReceiving objects:  40% (20/50)\rremote: Total 50 (delta 8), reused 45 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects:  42% (21/50)\rReceiving objects:  44% (22/50)\rReceiving objects:  46% (23/50)\rReceiving objects:  48% (24/50)\rReceiving objects:  50% (25/50)\rReceiving objects:  52% (26/50)\rReceiving objects:  54% (27/50)\rReceiving objects:  56% (28/50)\rReceiving objects:  58% (29/50)\rReceiving objects:  60% (30/50)\rReceiving objects:  62% (31/50)\rReceiving objects:  64% (32/50)\rReceiving objects:  66% (33/50)\rReceiving objects:  68% (34/50)\rReceiving objects:  70% (35/50)\rReceiving objects:  72% (36/50)\rReceiving objects:  74% (37/50)\rReceiving objects:  76% (38/50)\rReceiving objects:  78% (39/50)\rReceiving objects:  80% (40/50)\rReceiving objects:  82% (41/50)\rReceiving objects:  84% (42/50)\rReceiving objects:  86% (43/50)\rReceiving objects:  88% (44/50)\rReceiving objects:  90% (45/50)\rReceiving objects:  92% (46/50)\rReceiving objects:  94% (47/50)\rReceiving objects:  96% (48/50)\rReceiving objects:  98% (49/50)\rReceiving objects: 100% (50/50)\rReceiving objects: 100% (50/50), 154.63 KiB | 17.18 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n",
            "--2025-03-08 10:49:33--  https://raw.githubusercontent.com/HappyColor/Vesper/refs/heads/master/configs/model_config.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2477 (2.4K) [text/plain]\n",
            "Saving to: ‘model_config.py’\n",
            "\n",
            "model_config.py     100%[===================>]   2.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-08 10:49:33 (37.6 MB/s) - ‘model_config.py’ saved [2477/2477]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load dataset_config.py\n",
        "\n",
        "from yacs.config import CfgNode as CN\n",
        "\n",
        "_C = CN(new_allowed=True)\n",
        "\n",
        "###########\n",
        "# IEMOCAP #\n",
        "###########\n",
        "_C.iemocap = CN(new_allowed=True)\n",
        "_C.iemocap.num_classes = 4\n",
        "_C.iemocap.meta_csv_file = '/148Dataset/data-chen.weidong/iemocap/feature/name_label_text.csv'\n",
        "_C.iemocap.wavdir = '/148Dataset/data-chen.weidong/iemocap/wav_all_sentences'\n",
        "_C.iemocap.batch_length = 104000 # 16000 * 6.5\n",
        "_C.iemocap.evaluate = ['accuracy', 'recall']\n",
        "_C.iemocap.folds = [1, 2, 3, 4, 5]\n",
        "_C.iemocap.f1 = 'weighted'\n",
        "_C.iemocap.have_test_set = False\n",
        "\n",
        "########\n",
        "# MELD #\n",
        "########\n",
        "_C.meld = CN(new_allowed=True)\n",
        "_C.meld.num_classes = 7\n",
        "_C.meld.meta_csv_file = '/148Dataset/data-chen.weidong/meld/label/official'\n",
        "_C.meld.wavdir = '/148Dataset/data-chen.weidong/meld/audio_16k'\n",
        "_C.meld.batch_length = 72000 # 16000 * 4.5\n",
        "_C.meld.evaluate = ['f1']\n",
        "_C.meld.folds = [1]\n",
        "_C.meld.f1 = 'weighted'\n",
        "_C.meld.have_test_set = True\n",
        "\n",
        "###########\n",
        "# CREMA-D #\n",
        "###########\n",
        "_C.crema = CN(new_allowed=True)\n",
        "_C.crema.num_classes = 6\n",
        "_C.crema.meta_csv_file = '/148Dataset/data-chen.weidong/CREMA-D/CREMA-D.csv'\n",
        "_C.crema.wavdir = '/148Dataset/data-chen.weidong/CREMA-D/AudioWAV'\n",
        "_C.crema.batch_length = 48000 # 16000 * 3.0\n",
        "_C.crema.evaluate = ['accuracy', 'recall']\n",
        "_C.crema.folds = [1]\n",
        "_C.crema.f1 = 'weighted'\n",
        "_C.crema.have_test_set = False\n",
        "\n",
        "#########\n",
        "# LSSED #\n",
        "#########\n",
        "_C.lssed = CN(new_allowed=True)\n",
        "_C.lssed.num_classes = 4\n",
        "_C.lssed.meta_csv_file = '/148Dataset/data-chen.weidong/lssed_all/metadata_english_all.csv'\n",
        "_C.lssed.wavdir = '/148Dataset/data-chen.weidong/lssed_all/wav_all'\n",
        "_C.lssed.batch_length = 80000 # 16000*5\n",
        "_C.lssed.evaluate = ['accuracy', 'recall']\n",
        "_C.lssed.folds = [1]\n",
        "_C.lssed.f1 = 'weighted'\n",
        "_C.lssed.have_test_set = True\n",
        "\n",
        "_C.lssed.target_length = 249\n",
        "_C.lssed.l_target_dir = '/148Dataset/data-chen.weidong/lssed_all/feature/wavlm_large_L12_mat'\n",
        "_C.lssed.h_target_dir = '/148Dataset/data-chen.weidong/lssed_all/feature/wavlm_large_L24_mat'\n"
      ],
      "metadata": {
        "id": "FyBD5wypb7i7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建新文件夹（如 AudioMP3）\n",
        "!mkdir /content/wavdir"
      ],
      "metadata": {
        "id": "G949vk5ZlpM8"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 移动所有 MP3 文件到 AudioMP3 文件夹\n",
        "!mv /content/*.mp3 /content/wavdir/"
      ],
      "metadata": {
        "id": "NQWDIbYfl07L"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 音频文件目录\n",
        "audio_dir = \"/content/wavdir\"\n",
        "\n",
        "# 列出所有音频文件\n",
        "audio_files = [f for f in os.listdir(audio_dir) if f.endswith(\".wav\")]\n",
        "\n",
        "# 创建元数据\n",
        "data = {\"name\": audio_files}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 保存为 CSV 文件\n",
        "df.to_csv(\"/content/meta_csv_file.csv\", index=False)\n",
        "\n",
        "print(\"元数据文件已生成：/content/meta_csv_file.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_vitbXFmWqu",
        "outputId": "5d59bf31-000c-4c59-c73c-fb8e1b816976"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "元数据文件已生成：/content/meta_csv_file.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 带标签元数据\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 音频文件目录\n",
        "audio_dir = \"/content/wavdir\"\n",
        "\n",
        "# 列出所有音频文件\n",
        "audio_files = [f for f in os.listdir(audio_dir) if f.endswith(\".wav\")]\n",
        "\n",
        "# 解析文件名生成标签\n",
        "data = []\n",
        "for file in audio_files:\n",
        "    # 假设文件名格式为 \"audio1_anger.wav\"\n",
        "    name, label = file.rsplit(\"_\", 1)  # 从右边分割一次\n",
        "    label = label.replace(\".wav\", \"\")  # 去掉文件扩展名\n",
        "    data.append({\"name\": file, \"label\": label})\n",
        "\n",
        "# 创建 DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 保存为 CSV 文件\n",
        "df.to_csv(\"/content/metadata_english_all.csv\", index=False)\n",
        "\n",
        "print(\"元数据文件已生成：/content/metadata_english_all.csv\")"
      ],
      "metadata": {
        "id": "XL7WPKejm3PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 改变文件为可写模式\n",
        "!chmod 777 /content/Vesper/extract_feature/WavLM/extract_wavlm.py\n"
      ],
      "metadata": {
        "id": "qVLzv6rnuoqg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "提取预训练数据特征"
      ],
      "metadata": {
        "id": "APNk9JqdnRRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "ckpt_path = \"/content/pytorch_model.bin\"\n",
        "\n",
        "# 检查文件是否存在且非空\n",
        "if not os.path.exists(ckpt_path) or os.path.getsize(ckpt_path) == 0:\n",
        "    raise FileNotFoundError(f\"权重文件 {ckpt_path} 不存在或为空，请重新下载。\")\n",
        "\n",
        "# 尝试加载\n",
        "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "\n",
        "# 查看 checkpoint 的 keys\n",
        "print(checkpoint.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5Mv2Uw3yrkv",
        "outputId": "dfb7a5b0-c87e-4099-8a29-4678738dff2d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-9b8ee5cb1e69>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['masked_spec_embed', 'feature_extractor.conv_layers.0.conv.weight', 'feature_extractor.conv_layers.0.layer_norm.weight', 'feature_extractor.conv_layers.0.layer_norm.bias', 'feature_extractor.conv_layers.1.conv.weight', 'feature_extractor.conv_layers.2.conv.weight', 'feature_extractor.conv_layers.3.conv.weight', 'feature_extractor.conv_layers.4.conv.weight', 'feature_extractor.conv_layers.5.conv.weight', 'feature_extractor.conv_layers.6.conv.weight', 'feature_projection.layer_norm.weight', 'feature_projection.layer_norm.bias', 'feature_projection.projection.weight', 'feature_projection.projection.bias', 'encoder.pos_conv_embed.conv.bias', 'encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v', 'encoder.layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layers.0.attention.gru_rel_pos_const', 'encoder.layers.0.attention.k_proj.weight', 'encoder.layers.0.attention.k_proj.bias', 'encoder.layers.0.attention.v_proj.weight', 'encoder.layers.0.attention.v_proj.bias', 'encoder.layers.0.attention.q_proj.weight', 'encoder.layers.0.attention.q_proj.bias', 'encoder.layers.0.attention.out_proj.weight', 'encoder.layers.0.attention.out_proj.bias', 'encoder.layers.0.attention.gru_rel_pos_linear.weight', 'encoder.layers.0.attention.gru_rel_pos_linear.bias', 'encoder.layers.0.attention.rel_attn_embed.weight', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.feed_forward.intermediate_dense.weight', 'encoder.layers.0.feed_forward.intermediate_dense.bias', 'encoder.layers.0.feed_forward.output_dense.weight', 'encoder.layers.0.feed_forward.output_dense.bias', 'encoder.layers.0.final_layer_norm.weight', 'encoder.layers.0.final_layer_norm.bias', 'encoder.layers.1.attention.gru_rel_pos_const', 'encoder.layers.1.attention.k_proj.weight', 'encoder.layers.1.attention.k_proj.bias', 'encoder.layers.1.attention.v_proj.weight', 'encoder.layers.1.attention.v_proj.bias', 'encoder.layers.1.attention.q_proj.weight', 'encoder.layers.1.attention.q_proj.bias', 'encoder.layers.1.attention.out_proj.weight', 'encoder.layers.1.attention.out_proj.bias', 'encoder.layers.1.attention.gru_rel_pos_linear.weight', 'encoder.layers.1.attention.gru_rel_pos_linear.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.feed_forward.intermediate_dense.weight', 'encoder.layers.1.feed_forward.intermediate_dense.bias', 'encoder.layers.1.feed_forward.output_dense.weight', 'encoder.layers.1.feed_forward.output_dense.bias', 'encoder.layers.1.final_layer_norm.weight', 'encoder.layers.1.final_layer_norm.bias', 'encoder.layers.2.attention.gru_rel_pos_const', 'encoder.layers.2.attention.k_proj.weight', 'encoder.layers.2.attention.k_proj.bias', 'encoder.layers.2.attention.v_proj.weight', 'encoder.layers.2.attention.v_proj.bias', 'encoder.layers.2.attention.q_proj.weight', 'encoder.layers.2.attention.q_proj.bias', 'encoder.layers.2.attention.out_proj.weight', 'encoder.layers.2.attention.out_proj.bias', 'encoder.layers.2.attention.gru_rel_pos_linear.weight', 'encoder.layers.2.attention.gru_rel_pos_linear.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.feed_forward.intermediate_dense.weight', 'encoder.layers.2.feed_forward.intermediate_dense.bias', 'encoder.layers.2.feed_forward.output_dense.weight', 'encoder.layers.2.feed_forward.output_dense.bias', 'encoder.layers.2.final_layer_norm.weight', 'encoder.layers.2.final_layer_norm.bias', 'encoder.layers.3.attention.gru_rel_pos_const', 'encoder.layers.3.attention.k_proj.weight', 'encoder.layers.3.attention.k_proj.bias', 'encoder.layers.3.attention.v_proj.weight', 'encoder.layers.3.attention.v_proj.bias', 'encoder.layers.3.attention.q_proj.weight', 'encoder.layers.3.attention.q_proj.bias', 'encoder.layers.3.attention.out_proj.weight', 'encoder.layers.3.attention.out_proj.bias', 'encoder.layers.3.attention.gru_rel_pos_linear.weight', 'encoder.layers.3.attention.gru_rel_pos_linear.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.feed_forward.intermediate_dense.weight', 'encoder.layers.3.feed_forward.intermediate_dense.bias', 'encoder.layers.3.feed_forward.output_dense.weight', 'encoder.layers.3.feed_forward.output_dense.bias', 'encoder.layers.3.final_layer_norm.weight', 'encoder.layers.3.final_layer_norm.bias', 'encoder.layers.4.attention.gru_rel_pos_const', 'encoder.layers.4.attention.k_proj.weight', 'encoder.layers.4.attention.k_proj.bias', 'encoder.layers.4.attention.v_proj.weight', 'encoder.layers.4.attention.v_proj.bias', 'encoder.layers.4.attention.q_proj.weight', 'encoder.layers.4.attention.q_proj.bias', 'encoder.layers.4.attention.out_proj.weight', 'encoder.layers.4.attention.out_proj.bias', 'encoder.layers.4.attention.gru_rel_pos_linear.weight', 'encoder.layers.4.attention.gru_rel_pos_linear.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.feed_forward.intermediate_dense.weight', 'encoder.layers.4.feed_forward.intermediate_dense.bias', 'encoder.layers.4.feed_forward.output_dense.weight', 'encoder.layers.4.feed_forward.output_dense.bias', 'encoder.layers.4.final_layer_norm.weight', 'encoder.layers.4.final_layer_norm.bias', 'encoder.layers.5.attention.gru_rel_pos_const', 'encoder.layers.5.attention.k_proj.weight', 'encoder.layers.5.attention.k_proj.bias', 'encoder.layers.5.attention.v_proj.weight', 'encoder.layers.5.attention.v_proj.bias', 'encoder.layers.5.attention.q_proj.weight', 'encoder.layers.5.attention.q_proj.bias', 'encoder.layers.5.attention.out_proj.weight', 'encoder.layers.5.attention.out_proj.bias', 'encoder.layers.5.attention.gru_rel_pos_linear.weight', 'encoder.layers.5.attention.gru_rel_pos_linear.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.layers.5.feed_forward.output_dense.weight', 'encoder.layers.5.feed_forward.output_dense.bias', 'encoder.layers.5.final_layer_norm.weight', 'encoder.layers.5.final_layer_norm.bias', 'encoder.layers.6.attention.gru_rel_pos_const', 'encoder.layers.6.attention.k_proj.weight', 'encoder.layers.6.attention.k_proj.bias', 'encoder.layers.6.attention.v_proj.weight', 'encoder.layers.6.attention.v_proj.bias', 'encoder.layers.6.attention.q_proj.weight', 'encoder.layers.6.attention.q_proj.bias', 'encoder.layers.6.attention.out_proj.weight', 'encoder.layers.6.attention.out_proj.bias', 'encoder.layers.6.attention.gru_rel_pos_linear.weight', 'encoder.layers.6.attention.gru_rel_pos_linear.bias', 'encoder.layers.6.layer_norm.weight', 'encoder.layers.6.layer_norm.bias', 'encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.layers.6.feed_forward.output_dense.weight', 'encoder.layers.6.feed_forward.output_dense.bias', 'encoder.layers.6.final_layer_norm.weight', 'encoder.layers.6.final_layer_norm.bias', 'encoder.layers.7.attention.gru_rel_pos_const', 'encoder.layers.7.attention.k_proj.weight', 'encoder.layers.7.attention.k_proj.bias', 'encoder.layers.7.attention.v_proj.weight', 'encoder.layers.7.attention.v_proj.bias', 'encoder.layers.7.attention.q_proj.weight', 'encoder.layers.7.attention.q_proj.bias', 'encoder.layers.7.attention.out_proj.weight', 'encoder.layers.7.attention.out_proj.bias', 'encoder.layers.7.attention.gru_rel_pos_linear.weight', 'encoder.layers.7.attention.gru_rel_pos_linear.bias', 'encoder.layers.7.layer_norm.weight', 'encoder.layers.7.layer_norm.bias', 'encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.layers.7.feed_forward.output_dense.weight', 'encoder.layers.7.feed_forward.output_dense.bias', 'encoder.layers.7.final_layer_norm.weight', 'encoder.layers.7.final_layer_norm.bias', 'encoder.layers.8.attention.gru_rel_pos_const', 'encoder.layers.8.attention.k_proj.weight', 'encoder.layers.8.attention.k_proj.bias', 'encoder.layers.8.attention.v_proj.weight', 'encoder.layers.8.attention.v_proj.bias', 'encoder.layers.8.attention.q_proj.weight', 'encoder.layers.8.attention.q_proj.bias', 'encoder.layers.8.attention.out_proj.weight', 'encoder.layers.8.attention.out_proj.bias', 'encoder.layers.8.attention.gru_rel_pos_linear.weight', 'encoder.layers.8.attention.gru_rel_pos_linear.bias', 'encoder.layers.8.layer_norm.weight', 'encoder.layers.8.layer_norm.bias', 'encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.layers.8.feed_forward.output_dense.weight', 'encoder.layers.8.feed_forward.output_dense.bias', 'encoder.layers.8.final_layer_norm.weight', 'encoder.layers.8.final_layer_norm.bias', 'encoder.layers.9.attention.gru_rel_pos_const', 'encoder.layers.9.attention.k_proj.weight', 'encoder.layers.9.attention.k_proj.bias', 'encoder.layers.9.attention.v_proj.weight', 'encoder.layers.9.attention.v_proj.bias', 'encoder.layers.9.attention.q_proj.weight', 'encoder.layers.9.attention.q_proj.bias', 'encoder.layers.9.attention.out_proj.weight', 'encoder.layers.9.attention.out_proj.bias', 'encoder.layers.9.attention.gru_rel_pos_linear.weight', 'encoder.layers.9.attention.gru_rel_pos_linear.bias', 'encoder.layers.9.layer_norm.weight', 'encoder.layers.9.layer_norm.bias', 'encoder.layers.9.feed_forward.intermediate_dense.weight', 'encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.layers.9.feed_forward.output_dense.weight', 'encoder.layers.9.feed_forward.output_dense.bias', 'encoder.layers.9.final_layer_norm.weight', 'encoder.layers.9.final_layer_norm.bias', 'encoder.layers.10.attention.gru_rel_pos_const', 'encoder.layers.10.attention.k_proj.weight', 'encoder.layers.10.attention.k_proj.bias', 'encoder.layers.10.attention.v_proj.weight', 'encoder.layers.10.attention.v_proj.bias', 'encoder.layers.10.attention.q_proj.weight', 'encoder.layers.10.attention.q_proj.bias', 'encoder.layers.10.attention.out_proj.weight', 'encoder.layers.10.attention.out_proj.bias', 'encoder.layers.10.attention.gru_rel_pos_linear.weight', 'encoder.layers.10.attention.gru_rel_pos_linear.bias', 'encoder.layers.10.layer_norm.weight', 'encoder.layers.10.layer_norm.bias', 'encoder.layers.10.feed_forward.intermediate_dense.weight', 'encoder.layers.10.feed_forward.intermediate_dense.bias', 'encoder.layers.10.feed_forward.output_dense.weight', 'encoder.layers.10.feed_forward.output_dense.bias', 'encoder.layers.10.final_layer_norm.weight', 'encoder.layers.10.final_layer_norm.bias', 'encoder.layers.11.attention.gru_rel_pos_const', 'encoder.layers.11.attention.k_proj.weight', 'encoder.layers.11.attention.k_proj.bias', 'encoder.layers.11.attention.v_proj.weight', 'encoder.layers.11.attention.v_proj.bias', 'encoder.layers.11.attention.q_proj.weight', 'encoder.layers.11.attention.q_proj.bias', 'encoder.layers.11.attention.out_proj.weight', 'encoder.layers.11.attention.out_proj.bias', 'encoder.layers.11.attention.gru_rel_pos_linear.weight', 'encoder.layers.11.attention.gru_rel_pos_linear.bias', 'encoder.layers.11.layer_norm.weight', 'encoder.layers.11.layer_norm.bias', 'encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.layers.11.feed_forward.output_dense.weight', 'encoder.layers.11.feed_forward.output_dense.bias', 'encoder.layers.11.final_layer_norm.weight', 'encoder.layers.11.final_layer_norm.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WavLMModel, WavLMConfig\n",
        "\n",
        "# 使用 Hugging Face 加载模型和配置\n",
        "model = WavLMModel.from_pretrained(\"/content\")\n"
      ],
      "metadata": {
        "id": "59yNc-fSy-84"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Vesper/extract_feature/WavLM/extract_wavlm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ-PXqg6nQZh",
        "outputId": "4c5d17b2-a02e-4666-8d08-5dbf4a8b78cb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Vesper/extract_feature/WavLM/extract_wavlm.py\", line 119, in <module>\n",
            "    main(args)\n",
            "  File \"/content/Vesper/extract_feature/WavLM/extract_wavlm.py\", line 83, in main\n",
            "    cfg = WavLMConfig(checkpoint['cfg'])\n",
            "                      ~~~~~~~~~~^^^^^^^\n",
            "KeyError: 'cfg'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/Vesper/extract_feature/WavLM/WavLM-Base+.pt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJKDgzESvnQ0",
        "outputId": "17692894-3650-41f5-ca60-e72033fdaa8c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 0 Mar  8 12:08 /content/Vesper/extract_feature/WavLM/WavLM-Base+.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load /content/model_config.py\n",
        "\n",
        "from yacs.config import CfgNode as CN\n",
        "_C = CN(new_allowed=True)\n",
        "\n",
        "###############\n",
        "# Transformer #\n",
        "###############\n",
        "_C.Transformer = CN(new_allowed=True)\n",
        "\n",
        "_C.Transformer.num_encoder_layers = 4\n",
        "_C.Transformer.embed_dim = 1024\n",
        "_C.Transformer.ffn_embed_dim = 512\n",
        "_C.Transformer.num_heads = 8\n",
        "_C.Transformer.activation = 'gelu'\n",
        "_C.Transformer.dropout = 0.1\n",
        "_C.Transformer.bias = True\n",
        "_C.Transformer.normalize_before = True\n",
        "\n",
        "# positional embeddings\n",
        "_C.Transformer.conv_pos = 128\n",
        "_C.Transformer.conv_pos_groups = 16\n",
        "\n",
        "##########\n",
        "# Vesper #\n",
        "##########\n",
        "_C.Vesper = CN(new_allowed=True)\n",
        "\n",
        "# mainstream model\n",
        "_C.Vesper.encoder_layers= 4\n",
        "_C.Vesper.encoder_embed_dim = 1024\n",
        "_C.Vesper.ffn_embed_dim = 4096\n",
        "_C.Vesper.num_heads = 16\n",
        "_C.Vesper.activation = 'gelu'\n",
        "_C.Vesper.dropout = 0.1\n",
        "_C.Vesper.bias = True\n",
        "_C.Vesper.normalize = True\n",
        "_C.Vesper.normalize_before = True\n",
        "_C.Vesper.relative_position_embedding = True\n",
        "_C.Vesper.qk_norm = False  # query/key (QK) normalization\n",
        "\n",
        "# predictor\n",
        "_C.Vesper.enable_l_predictor = True\n",
        "_C.Vesper.enable_h_predictor = True\n",
        "_C.Vesper.enable_x_predictor = True\n",
        "\n",
        "# FinetuneWrapper\n",
        "_C.Vesper.projector_dim = 256\n",
        "_C.Vesper.output_rep = 'weighted_sum' # 'weighted_sum' / 'last_layer'\n",
        "\n",
        "# initiliaze with wavlm\n",
        "_C.Vesper.init_with_wavlm = True\n",
        "_C.Vesper.init_style = ['uniform_extract']   # ['custom_average', [(0, 1), (2, 5), (6, 13), (14, 23)]], ['custom_extract', [0, 5, 11, 17]]\n",
        "_C.Vesper.path_to_wavlm = ['/148Dataset/data-chen.weidong/pre_trained_model/wavlm/WavLM-Base.pt', '/148Dataset/data-chen.weidong/pre_trained_model/wavlm/WavLM-Large.pt']\n",
        "\n",
        "# initiliaze with other pre-trained model\n",
        "_C.Vesper.init_with_ckpt = False\n",
        "_C.Vesper.path_to_vesper = ''\n",
        "\n",
        "# rms-based mask\n",
        "_C.Vesper.mask_depend_on_rms = True\n",
        "_C.Vesper.frame_length = 400 # 16000 * 0.025\n",
        "_C.Vesper.hop_length = 320   # 16000 * 0.020\n",
        "_C.Vesper.span_space = 1\n",
        "_C.Vesper.h_up = 1.0\n",
        "_C.Vesper.h_down = 0.5\n",
        "_C.Vesper.l_up = 0.49\n",
        "_C.Vesper.l_down = 0.2\n",
        "_C.Vesper.small_span = 8\n",
        "_C.Vesper.num_small_span = 20\n",
        "_C.Vesper.large_span = 40\n",
        "_C.Vesper.num_large_span = 4\n",
        "_C.Vesper.max_mask_percentage = 0.64\n",
        "\n",
        "# positional embedding\n",
        "_C.Vesper.conv_pos = 128\n",
        "_C.Vesper.conv_pos_groups = 16\n",
        "\n",
        "# bucket relative position embedding\n",
        "_C.Vesper.num_buckets = 320\n",
        "_C.Vesper.max_distance = 800\n",
        "_C.Vesper.gru_rel_pos = True\n",
        "\n",
        "# feature encoder\n",
        "_C.Vesper.extractor_mode = 'layer_norm'   # 'default' / 'layer_norm'\n",
        "_C.Vesper.conv_feature_layers = '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2'\n",
        "_C.Vesper.dropout_input = 0.0\n"
      ],
      "metadata": {
        "id": "uvzPtDgfbK0z"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import Wav2Vec2FeatureExtractor, WavLMForSequenceClassification\n",
        "import soundfile as sf  # 用于加载音频文件\n",
        "\n",
        "# 加载 Vesper 模型（假设基于 WavLM）\n",
        "model_path = \"path/to/vesper-4\"  # 替换为实际模型路径\n",
        "model = WavLMForSequenceClassification.from_pretrained(model_path)\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_path)\n",
        "\n",
        "# 加载音频文件\n",
        "def load_audio(file_path):\n",
        "    audio_input, sampling_rate = sf.read(file_path)\n",
        "    return audio_input, sampling_rate\n",
        "\n",
        "# 测试音频文件路径\n",
        "audio_file = \"path/to/test_audio.wav\"  # 替换为实际音频文件路径\n",
        "audio_input, sampling_rate = load_audio(audio_file)\n",
        "\n",
        "# 提取特征\n",
        "inputs = feature_extractor(audio_input, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
        "\n",
        "# 推理\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "# 输出情绪类别\n",
        "emotion_classes = [\"anger\", \"happy\", \"sad\", \"neutral\"]  # 假设情绪类别\n",
        "print(f\"Predicted emotion: {emotion_classes[predicted_class]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "bcw379B14C_-",
        "outputId": "6fd5b37a-6253-4a5f-9b86-5b830fe6bf78"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to import transformers.models.wavlm.modeling_wavlm because of the following error (look up to see its traceback):\nDetected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wavlm/modeling_wavlm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mmodeling_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m from ...utils import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdpa_attention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msdpa_attention_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOSS_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m from .pytorch_utils import (  # noqa: F401\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_deformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeformableDetrForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeformableDetrForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_deformable_detr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_to_corners_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_scipy_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0m_check_cuda_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m_check_cuda_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtv_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;34m\"Detected that PyTorch and torchvision were compiled with different CUDA major versions. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d502e3504e7f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWav2Vec2FeatureExtractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWavLMForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoundfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf\u001b[0m  \u001b[0;31m# 用于加载音频文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 加载 Vesper 模型（假设基于 WavLM）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1806\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1807\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1803\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1805\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1807\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1817\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1820\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.wavlm.modeling_wavlm because of the following error (look up to see its traceback):\nDetected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 列出 sample_data 目录下的文件\n",
        "!ls /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDsNNxEnPAj2",
        "outputId": "0579f2ef-9bc3-4868-d1cb-9920d08de131"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_config.py  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 移动文件到工作目录\n",
        "!mv /content/sample_data/dataset_config.py /content/"
      ],
      "metadata": {
        "id": "6G3xDbWFP_aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psKJVVb0SYwt",
        "outputId": "3219b289-fbc7-429e-bc20-30eee92ea6a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_config.py  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/dataset_config.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c62IIbyfOxPo",
        "outputId": "daaf13c5-4ae2-46cd-b966-f9b2a4dc67bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: /content/dataset_config.py: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 上传音频文件\n",
        "uploaded = files.upload()\n",
        "audio_file = list(uploaded.keys())[0]  # 获取上传的文件名"
      ],
      "metadata": {
        "id": "rFJtjcA-4Emx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "运行推理"
      ],
      "metadata": {
        "id": "hgWHaLN_4G8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载音频文件\n",
        "audio_input, sampling_rate = load_audio(audio_file)\n",
        "\n",
        "# 提取特征\n",
        "inputs = feature_extractor(audio_input, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
        "\n",
        "# 推理\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "# 输出情绪类别\n",
        "print(f\"Predicted emotion: {emotion_classes[predicted_class]}\")"
      ],
      "metadata": {
        "id": "6hMXoLHq4Jlm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}